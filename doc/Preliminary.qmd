---
title: "Heatmap"
subtitle: "Preliminary - PSYC 593 group project"
format:
  html: default
date: today
author: "Howard Tan, Huiyu Ding, Hector Shi"
---

## Introduction:

The visual world is remarkably complex and often overwhelming in terms of the information it presents. Much of how we interact with our surroundings is based on visual information processing, which leads to distinct patterns of viewing behavior aimed at quickly and efficiently interpreting the environment. Due to the anatomical limitation of the retina, which restricts high-acuity vision to the fovea, we must constantly move our eyes to capture detailed visual information. These movements involve shifting foveal attention multiple times per second, where detailed data is acquired, punctuated by saccadic jumps between fixations. Research has consistently shown that eye movement can be influenced by both bottom-up processes (e.g., our natural tendency to focus on bright or moving stimuli) and top-down processes like how our current goals may guide our eye movements in a task like visual search (e.g. Yarbus, 1967; Castelhano et al., 2009; Loftus & Mackworth, 1978). The importance of these findings has led to an increased interest in using eye-tracking as a powerful tool in cognitive science research.

Eye movement data offer distinct advantages over brain imaging techniques and basic behavioral statistics. Eye-tracking is relatively easy to implement, more affordable than imaging methods, and provides valuable additional insights into cognitive processes such as memory (e.g., Hannula et al., 2010) and attention (e.g. Awh et al., 2012; van Zoest et al., 2004). In typical eye-tracking studies, participants sit in front of a display while their eye movements are recorded by a head-mounted or desk-mounted eye-tracker. The recorded eye movements are then mapped to the task the participant is performing, allowing for detailed analyses of time-sensitive viewing behavior and potential indices of underlying cognitive processes. Eye-tracking data typically consists of time-point samples which contain gaze coordinates (usually represented by x and y positions in a 2D space mapped onto the display), and are later used to compute measures of interest like fixation duration and saccadic amplitudes (the distance the eyes travel during a saccade, measured in pixels or visual degrees). However, there are different thresholds to determine what constitutes a fixation and what constitutes a saccade depending on the study and the phenomenon one is trying to study. Additionally, typical psychophysics experiments contain trial data organized into rows, with variables measured contained in the columns, and researchers often have a need to combine the two datasets which are organized differently. Although several R packages, such as eyetrackingR and gazeR, have been designed to process eye movement datasets, many researchers still rely on custom code to perform data analysis and generate visualizations from their data. This can be challenging for researchers new to eye-tracking, as each visualization may take a long time to hand-craft and they may be unsure how to create visualizations that align with standards in the literature or effectively illustrate their findings.

This leads to the main objective of our project:

How should we visualize eye-tracking data? Given eye movements made throughout the time course of a trial, how do we display the data per image, per participant, per condition, per experiment? A key aspect of viewing behavior that has garnered significant attention is gaze or fixation patterns (e.g., Ramey et al., 2020; Wynn et al., 2019), which refer to sequences of fixations over a specified timeframe. First, the eye-tracking data is usually turned into a more usable format for researchers, which tends to be aggregated information about fixations and saccades that occurred within a trial. After that, the most common way to visualize these patterns is through fixation heatmaps, where we would map out the density map of fixations in a given trial, where areas receiving a larger number of fixations (weighted by the duration of the aggregated fixations) should be distinctively colored compared to areas receiving no or fewer number of fixations. Some tools, like the gazeR package, even incorporate fixation duration into the heatmap’s color scheme. However, while heatmaps are useful for showing the overall distribution of fixations, they do not convey the **temporal sequence** of fixations clearly, and so information about the scanpaths is also lost. One of the major goals of this project is to address this limitation by developing a streamlined pipeline in R code that goes from from loading raw eye-tracking data to generating clear, comprehensive visualizations of eye movement behavior (such as graphs on fixation statistics, saccadic dispersion, or in the case of temporal analyses, a short video that shows the sequences of fixations and their accompanying scanpaths).

## Method:

Howard’s dataset:

Eye movements were recorded with a SR-Research Eyelink 1000 Plus desktop mount eye-tracker sampling at 1000 Hz. Eye movement data was collected for either eye, or from both eyes. Saccades were pre- defined using the Eyelink 1000 Plus’s default velocity and acceleration thresholds (30°/s and 8000°/s2). Fixations were defined by Eyelink 1000 Plus’s default inter-saccade periods. In the code we develop, we would like to be able to change this variable and use the result in all subsequent analyses, in case other researchers have disagreements regarding what constitutes fixations and saccades (and microsaccades).

The experimental stimuli were colored turtles facing either left or right, presented on a beach background (Figure 1). Stimuli were placed onto the display in a rectangular grid with jitter added. The beach background contained a wave in the center, and contained a water light reflection pattern on either the left or right side.

For the purposes of evaluating the effectiveness of our heatmaps, we can compare two experimental conditions. There was a between-subjects manipulation of instruction (free-view, search) and subjects were randomly assigned to an instruction condition. Both instruction conditions received the same within-subject manipulations and the exact same generated search arrays. There were five set sizes for the less-salient purple turtles (0, 2, 4, 8, 16) and two salient turtle colors (yellow, green). The results showed drastic differences in first fixation statistics, as well as many eye-movement pattern measures, and we hope that these results will be reflected in the time-sequenced heatmaps we generate. 

Figure 1. 
*Example of Display Stimuli Used in Study*
![Figure1](figure1.png)


*Note.* *Top panel.* Example of trial used in study. The screen was blank for a variable time at the beginning of a trial. A fixation cross would blink until the participant fixated on it for 300ms. The trial display was gaze contingent and, in the free-view condition, it remained visible for two seconds, whereas in the search condition, it remained visible until response or four seconds had elapsed. Bottom panel. Stimuli used in study.

The eye-tracker was calibrated using a 3x3 spatial array before the experiment, and a single-point calibration (drift correct) occurred between every block of trials. We would like to include a check for calibration parameters and report subject average errors and group average errors.

Subjects in the free-view condition were given the following instructions: “In this experiment, we are interested in studying how people explore images with their eyes. We will show you several images and we just want you to look around. We will record your eye movements so that we can study how you are exploring the images.”

Subjects in the search condition were given the following instructions: “This experiment requires you to visually search for a target object and make a decision about it. The target you will be searching for is a red turtle. Your task is to decide towards which side the turtle’s head is facing, by pressing the LEFT or RIGHT ARROW key on the button box. And please try to respond as fast and as accurately as possible.” Before each trial was a solid color display that had the average color of the beach background, followed by a blinking fixation cross subjects had to fixate on in order to trigger the trial display. This meant that the first fixation made before the display appears is not informative, and should be excluded from analysis. Afterwards, the free-view display remained on screen for two seconds. The search display remained on screen for four seconds or until a response was made, whichever occurred earlier.

## Variables:

All the eye-tracking data are text files where a majority of the rows are a single time point containing information about the participants’ gaze at that time, regardless of what trial they were on. From this we will need to generate a list of fixations as well as saccades using any custom defined thresholds. Then given that list of eye movements made within a trial, we need to combine it to the main data file, where each row corresponds to an entire trial in its entirety. That is, we are grouping and collapsing the eye-tracking data into a more usable format for many kinds of research. From our main data file, we can overlay the fixation data onto screenshots of the stimuli to create heatmaps, since \~20 participants will have viewed the same image, so they will each contribute one sequence of fixations.

## Plan of Analysis:

From the data we collected from the data viewer, we will select the interested variables and conduct a classic table in which all the rows represent trials and columns represent variables, including fixations and saccades. We will compare how gaze behavior differs between the free-view and search conditions by looking at measures such as fixation duration, first fixation statistics, and saccadic movements from the table. By these analyses, we should be able to compare temporal gaze patterns with temporal sequence and scan path.

Moreover, we will use these data to generate fixation heatmaps to help us better understand the density of fixation. This helps in visualizing where participants focused their gaze the most. We will use the GazeR package in R to help us differentiate the color by the duration of fixation. By overlaying heatmaps from different conditions, we might visually highlight differences in fixation density and focus areas between free-viewing and search tasks.
